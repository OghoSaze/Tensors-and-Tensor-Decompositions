{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mKlmGX7Ibr_v"
      },
      "source": [
        "# Problem Sheet 8 - Tensors and Tensor Decompositions\n",
        "\n",
        "In this exercise we consider tensors and tensor decompositions. We work with tensorly and parts of this notebook have been adapted from https://github.com/JeanKossaifi/tensorly-notebooks"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install tensorly"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AnCyBOJAMMh2",
        "outputId": "c96b4213-e257-4f17-9602-f4b7d02b6748"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tensorly\n",
            "  Downloading tensorly-0.8.1-py3-none-any.whl (229 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m229.7/229.7 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from tensorly) (1.22.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from tensorly) (1.10.1)\n",
            "Installing collected packages: tensorly\n",
            "Successfully installed tensorly-0.8.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "_2b53wof8sCR"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorly as tl"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SaWvUAEg9C8-"
      },
      "source": [
        "# 1. Creating a tensor\n",
        "\n",
        "A tensor can be represented in multiple ways. The simplest is the slice representation through multiple matrices.\n",
        "\n",
        "Let's take for this example the tensor $\\tilde X$ defined by its frontal slices:\n",
        "\n",
        "$$\n",
        "   X_1 = \n",
        "   \\left[\n",
        "   \\begin{matrix}\n",
        "   0  & 2  & 4  & 6\\\\\n",
        "   8  & 10 & 12 & 14\\\\\n",
        "   16 & 18 & 20 & 22\n",
        "   \\end{matrix}\n",
        "   \\right]\n",
        "$$\n",
        "\n",
        "and \n",
        "\n",
        "$$\n",
        "   X_2 =\n",
        "   \\left[\n",
        "   \\begin{matrix}\n",
        "   1  & 3  & 5  & 7\\\\\n",
        "   9  & 11 & 13 & 15\\\\\n",
        "   17 & 19 & 21 & 23\n",
        "   \\end{matrix}\n",
        "   \\right]\n",
        "$$\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CqPKdviQ8szJ",
        "outputId": "521384c6-dc23-4174-ab34-a1235ea3e8c8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "In Python, this array can be expressed as a numpy array and converted to a tensorly tensor:\n",
            "[[[ 0.  1.]\n",
            "  [ 2.  3.]\n",
            "  [ 4.  5.]\n",
            "  [ 6.  7.]]\n",
            "\n",
            " [[ 8.  9.]\n",
            "  [10. 11.]\n",
            "  [12. 13.]\n",
            "  [14. 15.]]\n",
            "\n",
            " [[16. 17.]\n",
            "  [18. 19.]\n",
            "  [20. 21.]\n",
            "  [22. 23.]]]\n"
          ]
        }
      ],
      "source": [
        "print('In Python, this array can be expressed as a numpy array and converted to a tensorly tensor:')\n",
        "\n",
        "X = tl.tensor(np.arange(24).reshape((3, 4, 2)), dtype=tl.float64)\n",
        "print(X)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0hR-q4vELKHh"
      },
      "source": [
        "**Task:** Recall the definitions of fibers and slices and look at the tensor through them."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y8Y1QBKH94kE"
      },
      "source": [
        "# 2. Basic tensor operations\n",
        "## 2.1 Matrization/Unfolding\n",
        "\n",
        "You have learned about matrization/unfolding in the lecture.\n",
        "It turns out that there's an alternative (computationally more efficient) convention to do this that is implemented in the python library ``tensorly``.\n",
        "\n",
        "**Task:** Read about the two different conventions here: \n",
        "\n",
        "http://jeankossaifi.com/blog/unfolding.html\n",
        "\n",
        "The definition you know from the lecture is referred to as \"Kolda\".\n",
        "Consider the tensor $\\tilde X$ previously defined by\n",
        "$$\n",
        "   X_1 = \n",
        "   \\left[\n",
        "   \\begin{matrix}\n",
        "   0  & 2  & 4  & 6\\\\\n",
        "   8  & 10 & 12 & 14\\\\\n",
        "   16 & 18 & 20 & 22\n",
        "   \\end{matrix}\n",
        "   \\right]\n",
        "$$\n",
        "\n",
        "and \n",
        "\n",
        "$$\n",
        "   X_2 =\n",
        "   \\left[\n",
        "   \\begin{matrix}\n",
        "   1  & 3  & 5  & 7\\\\\n",
        "   9  & 11 & 13 & 15\\\\\n",
        "   17 & 19 & 21 & 23\n",
        "   \\end{matrix}\n",
        "   \\right]\n",
        "$$\n",
        "\n",
        "A possible implementation of the \"Kolda\" unfolding is given here:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "zsGmxFZGLKHi"
      },
      "outputs": [],
      "source": [
        "def reorder(indices, mode):\n",
        "    \"\"\"Reorders the elements\n",
        "    \"\"\"\n",
        "    indices = list(indices)\n",
        "    element = indices.pop(mode)\n",
        "    return ([element] + indices[::-1])\n",
        "\n",
        "def unfold_Kolda(tensor, mode):\n",
        "    \"\"\"Returns the mode-`mode` unfolding of `tensor`\n",
        "    \"\"\"\n",
        "    return np.transpose(tensor, reorder(range(tensor.ndim), mode)).reshape((tensor.shape[mode], -1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tAZy5Ls0LKHi"
      },
      "source": [
        "Play with unfolding $\\tilde X$ using the two different conventions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lxJj8EyrLKHi",
        "outputId": "13814331-11d3-4108-ba65-4ef05955cca5"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 0.,  2.,  4.,  6.,  1.,  3.,  5.,  7.],\n",
              "       [ 8., 10., 12., 14.,  9., 11., 13., 15.],\n",
              "       [16., 18., 20., 22., 17., 19., 21., 23.]])"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "unfold_Kolda(X, mode=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Udj0aP7_9JLg",
        "outputId": "6092e1d2-5ccc-44a1-e254-217a396a9787"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.],\n",
              "       [ 8.,  9., 10., 11., 12., 13., 14., 15.],\n",
              "       [16., 17., 18., 19., 20., 21., 22., 23.]])"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "tl.unfold(X, mode=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ULKLMQvw_F2l"
      },
      "source": [
        "## 2.2 Folding\n",
        "\n",
        "Folding is the inverse operation: you can **fold** an unfolded tensor back from the matrix to a full tensor.\n",
        "Of course, the two conventions have to be implemented differently.\n",
        "You can use either the ``tensorly.fold`` function or the \"Kolda\" version given below.\n",
        "\n",
        "**Task:** Convince yourself that folding the unfolded tensor gives back the original tensor for both conventions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "CCr-d3txLKHk"
      },
      "outputs": [],
      "source": [
        "def fold_Kolda(unfolded, mode, shape):\n",
        "    \"\"\"Returns the folded tensor of shape `shape` from the `mode`-mode unfolding `unfolded`.\n",
        "    \"\"\"\n",
        "    unfolded_indices = reorder(range(len(shape)), mode)\n",
        "    original_shape = [shape[i] for i in unfolded_indices]\n",
        "    unfolded = unfolded.reshape(original_shape)\n",
        "\n",
        "    folded_indices = list(range(len(shape)-1, 0, -1))\n",
        "    folded_indices.insert(mode, 0)\n",
        "    return np.transpose(unfolded, folded_indices)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JjOaKoZH_FDx",
        "outputId": "3c8f3e33-55d1-456a-8e36-8edd81fb43fe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X:\n",
            "[[[ 0.  1.]\n",
            "  [ 2.  3.]\n",
            "  [ 4.  5.]\n",
            "  [ 6.  7.]]\n",
            "\n",
            " [[ 8.  9.]\n",
            "  [10. 11.]\n",
            "  [12. 13.]\n",
            "  [14. 15.]]\n",
            "\n",
            " [[16. 17.]\n",
            "  [18. 19.]\n",
            "  [20. 21.]\n",
            "  [22. 23.]]]\n",
            "unfolded:\n",
            "[[ 0.  1.  8.  9. 16. 17.]\n",
            " [ 2.  3. 10. 11. 18. 19.]\n",
            " [ 4.  5. 12. 13. 20. 21.]\n",
            " [ 6.  7. 14. 15. 22. 23.]]\n",
            "folded again:\n",
            "[[[ 0.  1.]\n",
            "  [ 2.  3.]\n",
            "  [ 4.  5.]\n",
            "  [ 6.  7.]]\n",
            "\n",
            " [[ 8.  9.]\n",
            "  [10. 11.]\n",
            "  [12. 13.]\n",
            "  [14. 15.]]\n",
            "\n",
            " [[16. 17.]\n",
            "  [18. 19.]\n",
            "  [20. 21.]\n",
            "  [22. 23.]]]\n"
          ]
        }
      ],
      "source": [
        "print('X:')\n",
        "print(X)\n",
        "original_shape = X.shape\n",
        "\n",
        "mode = 1\n",
        "unfolded = tl.unfold(X, mode)\n",
        "print('unfolded:')\n",
        "print(unfolded)\n",
        "\n",
        "folded = tl.fold(unfolded, mode, original_shape)\n",
        "print('folded again:')\n",
        "print(folded)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jyv-6SGxAFV9"
      },
      "source": [
        "## 2.3 n-mode product\n",
        "\n",
        "Good new: in this subsection, we don't have to worry about different conventions anymore! So let's use what ``TensorLy`` has in store.\n",
        "\n",
        "In TensorLy, all the tensor algebra functions are located in the `tensorly.tenalg` module. For the n-mode product, you will need to use the function `mode_dot` that works transparently for multiplying a tensor by a matrix or a vector along a given mode.\n",
        "\n",
        "#### Tensor times matrix\n",
        "\n",
        "With the tensor $\\tilde X$ of size (3, 4, 2) we defined previously, let's define a matrix M of size (5, 4) to multiply along the second mode:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NvD7lYpG_Hlo",
        "outputId": "dfb5a941-8b27-4501-c316-7091325a41bf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "M:\n",
            "[[ 0  1  2  3]\n",
            " [ 4  5  6  7]\n",
            " [ 8  9 10 11]\n",
            " [12 13 14 15]\n",
            " [16 17 18 19]]\n",
            "2-mode product of X and M, result:\n",
            "[[[  28.   34.]\n",
            "  [  76.   98.]\n",
            "  [ 124.  162.]\n",
            "  [ 172.  226.]\n",
            "  [ 220.  290.]]\n",
            "\n",
            " [[  76.   82.]\n",
            "  [ 252.  274.]\n",
            "  [ 428.  466.]\n",
            "  [ 604.  658.]\n",
            "  [ 780.  850.]]\n",
            "\n",
            " [[ 124.  130.]\n",
            "  [ 428.  450.]\n",
            "  [ 732.  770.]\n",
            "  [1036. 1090.]\n",
            "  [1340. 1410.]]]\n"
          ]
        }
      ],
      "source": [
        "M = tl.tensor(np.arange(4*5).reshape((5, 4)))\n",
        "print('M:')\n",
        "print(M)\n",
        "result = tl.tenalg.mode_dot(X, M, mode=1)\n",
        "print('2-mode product of X and M, result:')\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eQyALMESAcZE"
      },
      "source": [
        "#### Tensor times vector\n",
        "\n",
        "Similarly, we can contract along the second mode with a vector of size 4 (our tensor is of size (3, 4, 2))."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aB2V1CVHALrL",
        "outputId": "51514222-05db-44a9-ba35-0ff9ddbdb0a6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "v: [0 1 2 3]\n",
            "result:\n",
            "[[ 28.  34.]\n",
            " [ 76.  82.]\n",
            " [124. 130.]]\n"
          ]
        }
      ],
      "source": [
        "v = tl.tensor(np.arange(4))\n",
        "print('v:', v)\n",
        "result = tl.tenalg.mode_dot(X, v, mode=1)\n",
        "print('result:')\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cUQCj-OaAhv-",
        "outputId": "552e3785-cbcf-4748-c736-3831af6a496b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Here, we could form each column (fiber) of the result, by taking the dot product between the frontal slices of X and the vector:\n",
            "[ 28.  76. 124.]\n",
            "[ 34.  82. 130.]\n",
            "And we can form the whole product by concanating these as the column of the resulting matrix:\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 28.,  34.],\n",
              "       [ 76.,  82.],\n",
              "       [124., 130.]])"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "print('Here, we could form each column (fiber) of the result, by taking the dot product between the frontal slices of X and the vector:')\n",
        "print(X[:, :, 0] @ v)\n",
        "print(X[:, :, 1] @ v)\n",
        "print('And we can form the whole product by concanating these as the column of the resulting matrix:')\n",
        "np.stack([X[:, :, 0] @ v, X[:, :, 1] @ v]).T"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PSksPDQUBZBd",
        "outputId": "7b90c65f-2dfa-4f1b-da77-ee4e18b8fe32"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "We could equivalently use the mode-0 slices:\n",
            "[28. 34.]\n",
            "[76. 82.]\n",
            "[124. 130.]\n",
            "And we can form the whole result by stacking these as the rows of the matrix:\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 28.,  34.],\n",
              "       [ 76.,  82.],\n",
              "       [124., 130.]])"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "print('We could equivalently use the mode-0 slices:')\n",
        "for i in range(3):\n",
        "    print(X[i, ...].T @ v)\n",
        "print('And we can form the whole result by stacking these as the rows of the matrix:')\n",
        "np.vstack([X[i, ...].T @ v for i in range(3)])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Dmat0WQUzz3"
      },
      "source": [
        "# 3. Khatri-Rao product\n",
        "\n",
        "Recall the Khatri-Rao product $A \\odot B$ of the two matrices $A \\in \\mathbb{R}^{I_1,J}$ and $B \\in \\mathbb{R}^{I_2,J}$. \n",
        "\n",
        "**Task:** Write a function that takes two matrices with the same number of columns as an input and returns their Khatri-Rao product."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "dDRcO0Z3LKHm"
      },
      "outputs": [],
      "source": [
        "def my_khatri_rao(A, B):\n",
        "    # Get the number of columns in A and B\n",
        "    num_cols = A.shape[1]\n",
        "\n",
        "    # Initialize an empty matrix for the Khatri-Rao product\n",
        "    R = np.empty((A.shape[0] * B.shape[0], num_cols))\n",
        "\n",
        "    # Compute the Khatri-Rao product column-wise\n",
        "    for col in range(num_cols):\n",
        "        R[:, col] = np.kron(A[:, col], B[:, col])\n",
        "\n",
        "    return R"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WPvIN2HILKHm"
      },
      "source": [
        "Compare the output of your function with that of TensorLy: ``tl.tenalg.khatri_rao`` to check if it works."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_t-N8-A2LKHm",
        "outputId": "16d6e0f5-50cb-48aa-bd0e-c2c269756496"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[  1,   4,   9],\n",
              "       [  4,  10,  18],\n",
              "       [  7,  16,  27],\n",
              "       [ 10,  22,  36],\n",
              "       [  4,  10,  18],\n",
              "       [ 16,  25,  36],\n",
              "       [ 28,  40,  54],\n",
              "       [ 40,  55,  72],\n",
              "       [  7,  16,  27],\n",
              "       [ 28,  40,  54],\n",
              "       [ 49,  64,  81],\n",
              "       [ 70,  88, 108]])"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "A = np.reshape(np.arange(1,10), (3,3))\n",
        "B = np.reshape(np.arange(1,13), (4,3))\n",
        "tl.tenalg.khatri_rao((A, B))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U_iCMyOdX1WL"
      },
      "source": [
        "# 4. Canonical Polyadic (CP) decomposition"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Nb9RhbVX4g9"
      },
      "source": [
        "In the lecture, you learned about the CP (Candecomp, Parafac/Canonical Polyadic) decomposition, which returns a so-called Kruskal form of a tensor\n",
        "$$\n",
        "\\tilde X = \\sum_{r=1}^R \\lambda_r a_r^{(1)} \\circ a_r^{(2)} \\circ a_r^{(3)}.\n",
        "$$\n",
        "Computing the rank $R$ of a tensor is an NP-hard problem, but we can use the CP decomposition as an approximation when choosing a rank for the result. \n",
        "It can be computed by an Alternating Least Squares (ALS) scheme.\n",
        "\n",
        "**Task:** Complete the code below, which computes the CP using ALS for a tensor with 3 modes.\n",
        "\n",
        "**Hint:** You can find two ways to do this in the lecture notes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "UHoTwQSNUqXO"
      },
      "outputs": [],
      "source": [
        "def my_cp_decomposition_als(tensor_X, rank_R, num_iter):\n",
        "    #initialize A1, A2, A3 using left leading singular vectors\n",
        "    X1 = tl.unfold(tensor_X, mode=0)\n",
        "    A1 = np.linalg.svd(X1)[0][:,:rank_R]\n",
        "    X2 = tl.unfold(tensor_X, mode=1)\n",
        "    A2 = np.linalg.svd(X2)[0][:,:rank_R]\n",
        "    X3 = tl.unfold(tensor_X, mode=2)\n",
        "    A3 = np.linalg.svd(X3)[0][:,:rank_R]\n",
        "    \n",
        "    for iter in range(num_iter):\n",
        "        A1 = A1 # CHANGE!\n",
        "        lambda_array = np.linalg.norm(A1, axis=0)\n",
        "        for i in range(rank_R):\n",
        "            A1[:,i] = A1[:,i] / lambda_array[i]\n",
        "        A2 = A2 # CHANGE!\n",
        "        lambda_array = np.linalg.norm(A2, axis=0)\n",
        "        for i in range(rank_R):\n",
        "            A2[:,i] = A2[:,i] / lambda_array[i]\n",
        "        A3 = A3 # CHANGE!\n",
        "        lambda_array = np.linalg.norm(A3, axis=0)\n",
        "        for i in range(rank_R):\n",
        "            A3[:,i] = A3[:,i] / lambda_array[i]\n",
        "    return lambda_array, (A1,A2,A3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HAJbGfYZLKHn",
        "outputId": "79666481-66f8-4f7f-ecd2-f0f31a65b928"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X: [[[ 0.  1.]\n",
            "  [ 2.  3.]\n",
            "  [ 4.  5.]\n",
            "  [ 6.  7.]]\n",
            "\n",
            " [[ 8.  9.]\n",
            "  [10. 11.]\n",
            "  [12. 13.]\n",
            "  [14. 15.]]\n",
            "\n",
            " [[16. 17.]\n",
            "  [18. 19.]\n",
            "  [20. 21.]\n",
            "  [22. 23.]]]\n",
            "\n",
            "Reconstruction from CP decomposition:\n",
            "[[[ 0.43796008 -0.50009678]\n",
            "  [ 0.14086623 -0.23613345]\n",
            "  [-0.15622763  0.02782987]\n",
            "  [-0.45332148  0.2917932 ]]\n",
            "\n",
            " [[ 0.01261587 -0.28672318]\n",
            "  [-0.09994788 -0.2264667 ]\n",
            "  [-0.21251164 -0.16621022]\n",
            "  [-0.3250754  -0.10595374]]\n",
            "\n",
            " [[-0.41272833 -0.07334958]\n",
            "  [-0.34076199 -0.21679995]\n",
            "  [-0.26879565 -0.36025032]\n",
            "  [-0.19682931 -0.50370068]]]\n"
          ]
        }
      ],
      "source": [
        "print('X:', X)\n",
        "ww, ff = my_cp_decomposition_als(X, 2, 20)\n",
        "full_tensor = tl.kruskal_to_tensor((ww, ff))\n",
        "print('\\nReconstruction from CP decomposition:')\n",
        "print(full_tensor)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L-wAhQLuLKHn"
      },
      "source": [
        "Compare the results of your implementation with Tensorly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "owYWTXHMLKHn",
        "outputId": "265da47c-1a9c-4621-8d70-1674a346feda"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[[-0.05785963  1.06474682]\n",
            "  [ 1.99072479  3.01009381]\n",
            "  [ 4.03930922  4.95544081]\n",
            "  [ 6.08789364  6.9007878 ]]\n",
            "\n",
            " [[ 8.02013459  8.98316657]\n",
            "  [10.01080986 10.99006258]\n",
            "  [12.00148513 12.9969586 ]\n",
            "  [13.9921604  15.00385461]]\n",
            "\n",
            " [[16.09812881 16.90158632]\n",
            "  [18.03089492 18.97003135]\n",
            "  [19.96366103 21.03847639]\n",
            "  [21.89642715 23.10692142]]]\n"
          ]
        }
      ],
      "source": [
        "from tensorly.decomposition import parafac\n",
        "weights, factors = parafac(X, rank=2)\n",
        "full_tensor = tl.kruskal_to_tensor((weights, factors))\n",
        "print(full_tensor)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L74RSJMLLKHo"
      },
      "source": [
        "# Additional task: Tucker Decomposition a.k.a. HOSVD\n",
        "\n",
        "**Task:** Implement the higher-order SVD (HOSVD) for 3-mode tensors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "j-Ua9rmyLKHo"
      },
      "outputs": [],
      "source": [
        "def my_hosvd(tensor_X, ranks):\n",
        "    unfolded_modes = [tl.unfold(tensor_X, mode) for mode in range(3)]\n",
        "\n",
        "    # Compute the SVD for each unfolded mode\n",
        "    factor_matrices = [np.linalg.svd(unfolded, full_matrices=False)[0] for unfolded in unfolded_modes]\n",
        "\n",
        "    # Reconstruct the core tensor\n",
        "    core_tensor = tl.tenalg.multi_mode_dot(tensor_X, factor_matrices, transpose=True)\n",
        "\n",
        "    # Extract the factor matrices\n",
        "    A1, A2, A3 = factor_matrices\n",
        "\n",
        "    return core_tensor, A1, A2, A3\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DL5bHYirLKHo"
      },
      "source": [
        "If implemented correctly, you should be able to execute the following code:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jfv3_rj3LKHo",
        "outputId": "bfa73172-acd5-4bf5-b5e5-0152b529e40c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Core tensor G:\n",
            "[[[-6.55274958e+01  2.81237824e-02]\n",
            "  [ 6.49654739e-03  3.14031179e-01]\n",
            "  [ 6.16146308e-15  1.04726206e-16]\n",
            "  [-5.54896385e-15  1.79850393e-15]]\n",
            "\n",
            " [[-2.68022035e-03 -1.17061732e+00]\n",
            "  [-5.34408458e+00 -3.43876182e-01]\n",
            "  [ 2.14368277e-15  1.79530866e-16]\n",
            "  [-1.31301759e-16 -1.30213746e-16]]\n",
            "\n",
            " [[-5.13731398e-15  1.17272774e-15]\n",
            "  [ 5.47004159e-15  7.98600625e-16]\n",
            "  [ 3.46382049e-16 -5.29650556e-17]\n",
            "  [ 3.66182703e-16  9.62504943e-16]]]\n",
            "\n",
            "Full tensor reconstructed from HOSVD:\n",
            "[[[4.69555017e-15 1.00000000e+00]\n",
            "  [2.00000000e+00 3.00000000e+00]\n",
            "  [4.00000000e+00 5.00000000e+00]\n",
            "  [6.00000000e+00 7.00000000e+00]]\n",
            "\n",
            " [[8.00000000e+00 9.00000000e+00]\n",
            "  [1.00000000e+01 1.10000000e+01]\n",
            "  [1.20000000e+01 1.30000000e+01]\n",
            "  [1.40000000e+01 1.50000000e+01]]\n",
            "\n",
            " [[1.60000000e+01 1.70000000e+01]\n",
            "  [1.80000000e+01 1.90000000e+01]\n",
            "  [2.00000000e+01 2.10000000e+01]\n",
            "  [2.20000000e+01 2.30000000e+01]]]\n"
          ]
        }
      ],
      "source": [
        "G,A1,A2,A3 = my_hosvd(X, [3, 4, 2])\n",
        "print('Core tensor G:')\n",
        "print(G)\n",
        "full_tensor = tl.tucker_to_tensor((G, [A1,A2,A3]))\n",
        "print('\\nFull tensor reconstructed from HOSVD:')\n",
        "print(full_tensor)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v1KsBmBzLKHp"
      },
      "source": [
        "Compare your results with Tensorly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ANkx45LYLKHp",
        "outputId": "fbceb0af-591a-4c1e-81d3-cf4ef5bcbda3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Core tensor G:\n",
            "[[[ 6.55274958e+01 -2.81237824e-02]\n",
            "  [-6.49654739e-03 -3.14031179e-01]\n",
            "  [ 8.44200415e-15  3.64395164e-16]\n",
            "  [-4.34527528e-15  1.26611803e-15]]\n",
            "\n",
            " [[-2.68022035e-03 -1.17061732e+00]\n",
            "  [-5.34408458e+00 -3.43876182e-01]\n",
            "  [-9.22296351e-16 -9.82315860e-17]\n",
            "  [-9.71645063e-16  6.31177723e-16]]\n",
            "\n",
            " [[ 5.57431378e-16 -1.18877633e-15]\n",
            "  [-2.97724080e-15  1.05265532e-17]\n",
            "  [-2.54129728e-16 -3.25113537e-16]\n",
            "  [ 6.97897582e-17  7.83396173e-16]]]\n",
            "\n",
            "Full tensor reconstructed from HOSVD:\n",
            "[[[-4.50674066e-15  1.00000000e+00]\n",
            "  [ 2.00000000e+00  3.00000000e+00]\n",
            "  [ 4.00000000e+00  5.00000000e+00]\n",
            "  [ 6.00000000e+00  7.00000000e+00]]\n",
            "\n",
            " [[ 8.00000000e+00  9.00000000e+00]\n",
            "  [ 1.00000000e+01  1.10000000e+01]\n",
            "  [ 1.20000000e+01  1.30000000e+01]\n",
            "  [ 1.40000000e+01  1.50000000e+01]]\n",
            "\n",
            " [[ 1.60000000e+01  1.70000000e+01]\n",
            "  [ 1.80000000e+01  1.90000000e+01]\n",
            "  [ 2.00000000e+01  2.10000000e+01]\n",
            "  [ 2.20000000e+01  2.30000000e+01]]]\n"
          ]
        }
      ],
      "source": [
        "from tensorly.decomposition import tucker\n",
        "core, factors = tucker(X, rank=[3, 4, 2])\n",
        "print('Core tensor G:')\n",
        "print(core)\n",
        "full_tensor = tl.tucker_to_tensor((core, factors))\n",
        "print('\\nFull tensor reconstructed from HOSVD:')\n",
        "print(full_tensor)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OGr9VLJ-SNVX"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python [conda env:mmds]",
      "language": "python",
      "name": "conda-env-mmds-py"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}